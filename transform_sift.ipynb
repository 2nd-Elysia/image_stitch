{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用于特征提取和图像匹配变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 特征提取\n",
    "def extract_features(image):\n",
    "    # image_array = np.array(image)\n",
    "    feature_extractor = cv2.SIFT_create()\n",
    "    keypoints, descriptors = feature_extractor.detectAndCompute(image, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "# 2. 特征匹配\n",
    "def match_features_bf(descriptors1, descriptors2):\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.5 * n.distance:\n",
    "            good_matches.append(m)\n",
    "    return good_matches\n",
    "\n",
    "def match_features_flann(descriptors1, descriptors2):\n",
    "    # FLANN 参数设置\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=50)\n",
    "    # 创建 FLANN 匹配器\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    # 使用 KNN 进行匹配\n",
    "    matches = flann.knnMatch(descriptors1, descriptors2, k=2)\n",
    "    # 选择好的匹配\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.5 * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    return good_matches\n",
    "# 注掉的方法是基于透视变换\n",
    "# # 3. 变换估计\n",
    "# def estimate_transform(keypoints1, keypoints2, good_matches):\n",
    "#     src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "#     dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "#     # 使用RANSAC算法估计变换矩阵\n",
    "#     M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "#     return M\n",
    "\n",
    "# #4. 变换应用\n",
    "# def apply_transform(image, M):\n",
    "#     # 应用估计的变换矩阵到图像\n",
    "#     transformed_image = cv2.warpPerspective(image, M, (image.shape[1], image.shape[0]))\n",
    "#     return transformed_image\n",
    "\n",
    "# 3. 仿射变换估计\n",
    "def estimate_transform(keypoints1, keypoints2, good_matches):\n",
    "    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    # 使用RANSAC算法估计仿射变换矩阵\n",
    "    M = cv2.estimateAffinePartial2D(src_pts, dst_pts)[0]\n",
    "    return M\n",
    "\n",
    "# 4. 仿射变换应用\n",
    "def apply_transform(image, M):\n",
    "    \n",
    "    # 应用估计的仿射变换矩阵到图像\n",
    "    # image = Image.fromarray(image)\n",
    "    transformed_image = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]))\n",
    "    return transformed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一次配准\n",
    "image1 = cv2.imread(r'origin_images_2nd\\B0.bmp', cv2.IMREAD_GRAYSCALE).astype(np.uint8)\n",
    "image2 = cv2.imread(r'origin_images_2nd\\B5.bmp', cv2.IMREAD_GRAYSCALE).astype(np.uint8)\n",
    "# array = np.load('anti_mean.npy')\n",
    "# image1 = np.clip(image1 / array,0,255)\n",
    "# image2 = np.clip(image2 / array,0,255)\n",
    "# image1 = image1.astype(np.uint8)\n",
    "# image2 = image2.astype(np.uint8)\n",
    "assert image1.shape == image2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# 1. 特征提取\n",
    "keypoints1, descriptors1 = extract_features(image1)\n",
    "keypoints2, descriptors2 = extract_features(image2)\n",
    "# 2. 特征匹配\n",
    "good_matches = match_features_bf(descriptors1, descriptors2)\n",
    "print(len(good_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\calib3d\\src\\ptsetreg.cpp:176: error: (-215:Assertion failed) count >= 0 && count2 == count in function 'cv::RANSACPointSetRegistrator::run'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32md:\\桌面\\Image_registration\\transform_sift.ipynb 单元格 5\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m good_matches \u001b[39m=\u001b[39m match_features_flann(descriptors1, descriptors2)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# 3. 变换估计\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m M \u001b[39m=\u001b[39m estimate_transform(keypoints1, keypoints2, good_matches)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# 4. 变换应用\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m transformed_image \u001b[39m=\u001b[39m apply_transform(image1, M)\n",
      "\u001b[1;32md:\\桌面\\Image_registration\\transform_sift.ipynb 单元格 5\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W4sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m dst_pts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfloat32([keypoints2[m\u001b[39m.\u001b[39mtrainIdx]\u001b[39m.\u001b[39mpt \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m good_matches])\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W4sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# 使用RANSAC算法估计仿射变换矩阵\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W4sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m M \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mestimateAffinePartial2D(src_pts, dst_pts)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W4sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mreturn\u001b[39;00m M\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\calib3d\\src\\ptsetreg.cpp:176: error: (-215:Assertion failed) count >= 0 && count2 == count in function 'cv::RANSACPointSetRegistrator::run'\n"
     ]
    }
   ],
   "source": [
    "# 3. 变换估计\n",
    "M = estimate_transform(keypoints1, keypoints2, good_matches)\n",
    "# 4. 变换应用\n",
    "transformed_image = apply_transform(image1, M)\n",
    "cv2.imwrite('trans_image/transformed2to222.png', transformed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 后面是SIFT和BFMatch合理性判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\桌面\\Image_registration\\transform_sift.ipynb 单元格 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#一次配准\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m image1 \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mimread(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mimage_test\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39ma.png\u001b[39;49m\u001b[39m'\u001b[39;49m, cv2\u001b[39m.\u001b[39;49mIMREAD_GRAYSCALE)\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39muint8)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m image2 \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mimage_test\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb.png\u001b[39m\u001b[39m'\u001b[39m, cv2\u001b[39m.\u001b[39mIMREAD_GRAYSCALE)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "#一次配准\n",
    "image1 = cv2.imread(r'image_test\\a.png', cv2.IMREAD_GRAYSCALE).astype(np.uint8)\n",
    "image2 = cv2.imread(r'image_test\\b.png', cv2.IMREAD_GRAYSCALE).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 特征提取\n",
    "keypoints1, descriptors1 = extract_features(image1)\n",
    "keypoints2, descriptors2 = extract_features(image2)\n",
    "matches = cv2.BFMatcher.knnMatch(descriptors1,descriptors2, k=2)\n",
    "good_matches = match_features_bf(descriptors1, descriptors2)\n",
    "img5 = cv2.drawMatchesKnn(image1,keypoints1,image2,keypoints2,matches,None,flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS | cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "pillow_image = Image.fromarray(cv2.cvtColor(img5, cv2.COLOR_BGR2RGB))\n",
    "pillow_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img6 = cv2.drawMatchesKnn(image1,keypoints1,image2,keypoints2,good,None,flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS | cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "pillow_image = Image.fromarray(cv2.cvtColor(img6, cv2.COLOR_BGR2RGB))\n",
    "pillow_image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
